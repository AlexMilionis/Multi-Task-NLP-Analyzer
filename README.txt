Developed a sophisticated coreference resolution system for the Winograd Schema Challenge using pre-trained Transformer models, including BERT, RoBERTa, and DistilBERT. These models were fine-tuned using Hugging Face and PyTorch to resolve ambiguous references within complex sentences. By leveraging the models' ability to capture deep contextual relationships in text, the system improved its understanding of challenging coreference tasks where the correct antecedent of a pronoun is determined by subtle shifts in meaning. Fine-tuning these models on Winograd-like data enabled a nuanced performance, enhancing their capability to handle diverse natural language inputs. The project focused on analyzing model predictions, refining results through extensive hyperparameter tuning, and using pre-trained language representations to push the boundaries of coreference resolution in Natural Language Processing.